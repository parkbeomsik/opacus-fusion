
/*
    Generated by generate_cutlass_code.py - Do not edit.
*/

#include "cutlass/cutlass.h"
#include "cutlass/gemm/gemm.h"
#include "cutlass/conv/convolution.h"
#include "cutlass/conv/conv2d_problem_size.h"
#include "cutlass/conv/kernel/implicit_gemm_convolution_grouped.h"
#include "cutlass/conv/kernel/default_conv2d_wgrad.h"
#include "cutlass/conv/kernel/default_conv2d_wgrad_grouped.h"
#include "cutlass/conv/device/implicit_gemm_convolution_grouped.h"
#include "cutlass/conv/device/implicit_gemm_convolution.h"

////////////////////////////////////////////////////////////////////

int main() {

  using cutlass_simt_swgrad_grouped_optimized_128x128x8_128x128x8_1x1x1_5_nhwc = typename cutlass::conv::kernel::DefaultConv2dWgradGrouped<
      float, 
      cutlass::layout::TensorNHWC,
      float,
      cutlass::layout::TensorNHWC,
      float, cutlass::layout::TensorNHWC,
      float, 
      cutlass::arch::OpClassSimt, 
      cutlass::arch::Sm80,
      cutlass::gemm::GemmShape<128, 128, 8>,
      cutlass::gemm::GemmShape<128, 128, 8>,
      cutlass::gemm::GemmShape<1, 1, 1>,
      cutlass::epilogue::thread::LinearCombination<
          float, 1,
          float, float>,
      cutlass::gemm::threadblock::GemmBatchedIdentityThreadblockSwizzle, 
      5,
      cutlass::arch::OpMultiplyAdd,
      cutlass::conv::IteratorAlgorithm::kOptimized
      >::Conv2dWgradKernel;

  using test = cutlass::conv::device::ImplicitGemmConvolutionGrouped<
      cutlass_simt_swgrad_grouped_optimized_128x128x8_128x128x8_1x1x1_5_nhwc>;

  return 0;

}

///////////////////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////////////////
    